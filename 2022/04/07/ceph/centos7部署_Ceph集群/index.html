<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="CentOS7部署_Ceph集群"><meta name="keywords" content="ceph"><meta name="author" content="YuiKuen.Yuen"><meta name="copyright" content="YuiKuen.Yuen"><title>CentOS7部署_Ceph集群 | Mr.Yuen'Blog</title><link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/yuikuen/picgo-cdn_images/img/favicon.png"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?1359876433997feef3afed2ab6c5b39c";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  hexoVersion: '6.1.0'
} </script><meta name="generator" content="Hexo 6.1.0"></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83"><span class="toc-number">1.</span> <span class="toc-text">配置环境</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%83%A8%E7%BD%B2%E5%B7%A5%E5%85%B7"><span class="toc-number">2.</span> <span class="toc-text">部署工具</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E9%9B%86%E7%BE%A4"><span class="toc-number">3.</span> <span class="toc-text">创建集群</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%8A%82%E7%82%B9%E5%8A%A0%E5%85%A5"><span class="toc-number">4.</span> <span class="toc-text">节点加入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA-mon-%E7%9B%91%E6%8E%A7"><span class="toc-number">5.</span> <span class="toc-text">创建 mon 监控</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA-mgr-%E7%AE%A1%E7%90%86"><span class="toc-number">6.</span> <span class="toc-text">创建 mgr 管理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA-osd-%E5%AD%98%E5%82%A8"><span class="toc-number">7.</span> <span class="toc-text">创建 osd 存储</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9B%86%E7%BE%A4%E6%89%A9%E5%AE%B9"><span class="toc-number">8.</span> <span class="toc-text">集群扩容</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Ceph-Dashboard"><span class="toc-number">9.</span> <span class="toc-text">Ceph-Dashboard</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Ceph-%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8"><span class="toc-number">10.</span> <span class="toc-text">Ceph 文件存储</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Ceph-%E5%9D%97%E5%AD%98%E5%82%A8"><span class="toc-number">11.</span> <span class="toc-text">Ceph 块存储</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA"><span class="toc-number">11.1.</span> <span class="toc-text">创建</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%89%A9%E5%AE%B9"><span class="toc-number">11.2.</span> <span class="toc-text">扩容</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BC%A9%E5%AE%B9"><span class="toc-number">11.3.</span> <span class="toc-text">缩容</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%A0%E9%99%A4"><span class="toc-number">11.4.</span> <span class="toc-text">删除</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Ceph-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8"><span class="toc-number">12.</span> <span class="toc-text">Ceph 对象存储</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://cdn.jsdelivr.net/gh/yuikuen/picgo-cdn_images/img/20220407100042.png"></div><div class="author-info__name text-center">YuiKuen.Yuen</div><div class="author-info__description text-center"></div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/yuikuen">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">128</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">55</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">26</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://molunerfinn.com">Molunerfinn</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://cdn.jsdelivr.net/gh/yuikuen/picgo-cdn_images/img/wallhaven-wqx3e7-tuya.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Mr.Yuen'Blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a><a class="site-page" href="/about">About</a></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title">CentOS7部署_Ceph集群</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2022-04-07</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/ceph/">ceph</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">4.6k</span><span class="post-meta__separator">|</span><span>Reading time: 22 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p><strong>版本环境：</strong></p>
<ul>
<li>硬件系统：ESXI 6.7.0 Update 1</li>
<li>操作系统：CentOS 7.9</li>
<li>Docker 版本：20.10.6</li>
<li>Ceph 版本：13.2.10</li>
</ul>
<blockquote>
<p><strong>集群环境：</strong>四台服务器，可上外网，其中三台作为 ceph 集群，一台作为 cehp 客户端，除了 client 外每台加一个磁盘（&#x2F;dev&#x2F;sdb），无需分区；</p>
</blockquote>
<h3 id="配置环境"><a href="#配置环境" class="headerlink" title="配置环境"></a>配置环境</h3><p>1）配置主机名及 hosts</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">cat</span> &gt;&gt; /etc/hosts &lt;&lt; <span class="string">EOF</span></span><br><span class="line"><span class="string">188.188.4.110 client</span></span><br><span class="line"><span class="string">188.188.4.111 ceph01</span></span><br><span class="line"><span class="string">188.188.4.112 ceph02</span></span><br><span class="line"><span class="string">188.188.4.113 ceph03</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure>

<p>2）关闭防火墙及 Selinux</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ systemctl <span class="built_in">disable</span> --now firewalld</span><br><span class="line">$ setenforce 0</span><br><span class="line">$ sed -i <span class="string">&#x27;s#SELINUX=enforcing#SELINUX=disabled#g&#x27;</span> /etc/sysconfig/selinux</span><br><span class="line">$ sed -i <span class="string">&#x27;s#SELINUX=enforcing#SELINUX=disabled#g&#x27;</span> /etc/selinux/config</span><br></pre></td></tr></table></figure>

<p>3）设置 ntpdate 时间同步并设置在 crontab 内</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ rpm -ivh http://mirrors.wlnmp.com/centos/wlnmp-release-centos.noarch.rpm</span><br><span class="line">$ yum install ntpdate -y</span><br><span class="line">$ <span class="built_in">ln</span> -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime</span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&#x27;Asia/Shanghai&#x27;</span> &gt;/etc/timezone</span><br><span class="line">$ ntpdate time2.aliyun.com</span><br><span class="line">$ crontab -e</span><br><span class="line"><span class="comment"># 加入到crontab</span></span><br><span class="line">*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com</span><br></pre></td></tr></table></figure>

<p>4）配置 Ceph-yum 源</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ yum install epel-release -y</span><br><span class="line">$ <span class="built_in">cat</span> &gt; /etc/yum.repos.d/ceph.repo &lt; EOF</span><br><span class="line">[ceph]</span><br><span class="line">name=ceph</span><br><span class="line">baseurl=http://mirrors.aliyun.com/ceph/rpm-mimic/el7/x86_64/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line">priority=1</span><br><span class="line"> </span><br><span class="line">[ceph-noarch]</span><br><span class="line">name=cephnoarch</span><br><span class="line">baseurl=http://mirrors.aliyun.com/ceph/rpm-mimic/el7/noarch/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line">priority=1</span><br><span class="line"> </span><br><span class="line">[ceph-source]</span><br><span class="line">name=Ceph <span class="built_in">source</span> packages</span><br><span class="line">baseurl=http://mirrors.aliyun.com/ceph/rpm-mimic/el7/SRPMS</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line">priority=1</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<p>5）更新系统并配置 ssh 免密登录（以 ceph01 为部署节点，配置使 ceph01 可以免密登录四台主机）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ yum -y update</span><br><span class="line">$ ssh-keygen -t rsa</span><br><span class="line">$ ssh-copy-id ceph01</span><br><span class="line">$ ssh-copy-id ceph02</span><br><span class="line">$ ssh-copy-id ceph03</span><br><span class="line">$ ssh-copy-id client</span><br></pre></td></tr></table></figure>

<p>在 ceph01 上ssh其他主机无需输入密码代表配置 ssh 成功</p>
<h3 id="部署工具"><a href="#部署工具" class="headerlink" title="部署工具"></a>部署工具</h3><p>1）在 ceph01 上安装部署工具（其他节点不用安装）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ yum install ceph-deploy -y</span><br></pre></td></tr></table></figure>

<p>2）每个节点都建立一个集群配置目录</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">mkdir</span> /etc/ceph</span><br><span class="line">$ <span class="built_in">cd</span> /etc/ceph</span><br></pre></td></tr></table></figure>

<h3 id="创建集群"><a href="#创建集群" class="headerlink" title="创建集群"></a>创建集群</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ceph-deploy new ceph01</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;/usr/bin/ceph-deploy&quot;</span>, line 18, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    from ceph_deploy.cli import main</span><br><span class="line">  File <span class="string">&quot;/usr/lib/python2.7/site-packages/ceph_deploy/cli.py&quot;</span>, line 1, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    import pkg_resources</span><br><span class="line">ImportError: No module named pkg_resources</span><br><span class="line"><span class="comment"># 报错处理，缺少python依赖</span></span><br><span class="line">$ yum install python-setuptools</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 成功执行过程如下：</span></span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /usr/bin/ceph-deploy new ceph01</span><br><span class="line">[ceph_deploy.cli][INFO  ] ceph-deploy options:</span><br><span class="line">[ceph_deploy.cli][INFO  ]  username                      : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  func                          : &lt;<span class="keyword">function</span> new at 0x7f9f8a0a1de8&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  verbose                       : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  overwrite_conf                : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  quiet                         : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9f8981b518&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cluster                       : ceph</span><br><span class="line">[ceph_deploy.cli][INFO  ]  ssh_copykey                   : True</span><br><span class="line">[ceph_deploy.cli][INFO  ]  mon                           : [<span class="string">&#x27;ceph01&#x27;</span>]</span><br><span class="line">[ceph_deploy.cli][INFO  ]  public_network                : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  ceph_conf                     : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cluster_network               : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  default_release               : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  fsid                          : None</span><br><span class="line">[ceph_deploy.new][DEBUG ] Creating new cluster named ceph                             <span class="comment"># 创建新的集群 ceph</span></span><br><span class="line">[ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds                       <span class="comment"># ssh 免密登录 ceph01 成功</span></span><br><span class="line">[ceph01][DEBUG ] connected to host: ceph01 </span><br><span class="line">[ceph01][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph01][DEBUG ] detect machine <span class="built_in">type</span></span><br><span class="line">[ceph01][DEBUG ] find the location of an executable</span><br><span class="line">[ceph01][INFO  ] Running <span class="built_in">command</span>: /usr/sbin/ip <span class="built_in">link</span> show</span><br><span class="line">[ceph01][INFO  ] Running <span class="built_in">command</span>: /usr/sbin/ip addr show</span><br><span class="line">[ceph01][DEBUG ] IP addresses found: [u<span class="string">&#x27;188.188.4.111&#x27;</span>]</span><br><span class="line">[ceph_deploy.new][DEBUG ] Resolving host ceph01                                       <span class="comment"># 连接ceph01获取IP为188.188.4.111</span></span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor ceph01 at 188.188.4.111</span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor initial members are [<span class="string">&#x27;ceph01&#x27;</span>]</span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor addrs are [<span class="string">&#x27;188.188.4.111&#x27;</span>]</span><br><span class="line">[ceph_deploy.new][DEBUG ] Creating a random mon key...                                <span class="comment"># 创建mon的key</span></span><br><span class="line">[ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...</span><br><span class="line">[ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...                      <span class="comment"># 创建ceph.conf配置文件</span></span><br></pre></td></tr></table></figure>

<p>会在当前配置文件目录生成以下配置文件</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph.conf  ceph-deploy-ceph.log  ceph.mon.keyring</span><br></pre></td></tr></table></figure>

<p>说明：</p>
<ul>
<li>ceph.conf                        集群配置文件</li>
<li>ceph-deploy-ceph.log   使用ceph-deploy部署的日志记录</li>
<li>ceph.mon.keyring          mon的验证key文件 监控需要的令牌</li>
</ul>
<h3 id="节点加入"><a href="#节点加入" class="headerlink" title="节点加入"></a>节点加入</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 所有节点都需要安装</span></span><br><span class="line">$ yum install ceph ceph-radosgw -y</span><br><span class="line">$ ceph -v</span><br><span class="line">ceph version 13.2.10 (564bdc4ae87418a232fc901524470e1a0f76d641) mimic (stable)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>注意： 如公网OK,并且网速好的话,可以用<code>ceph-deploy install node1 node2 node3</code>命令来安装,但网速不好的话会比较坑，所以这里选择直接用准备好的本地 ceph 源,然后<code>yum install ceph ceph-radosgw -y</code>安装即可。</p>
</blockquote>
<h3 id="创建-mon-监控"><a href="#创建-mon-监控" class="headerlink" title="创建 mon 监控"></a>创建 mon 监控</h3><blockquote>
<p>一般执行 <code>ceph-deploy mon create-initial</code> 即可，但执行报错，则修改配置 <code>public network = 188.188.4.0/24</code> 再执行命令</p>
</blockquote>
<p>1）解决命令执行错报错问题，修改 ceph01 配置文件在 <code>[global]</code> 配置端添加下面一句</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ vim ceph.conf </span><br><span class="line">public network = 188.188.4.0/24</span><br></pre></td></tr></table></figure>

<p>2）监控节点初始化，并同步配置到所有节点(ceph01,ceph02,ceph03,不包括 client）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ceph-deploy mon create-initial</span><br><span class="line">[node1][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf</span><br><span class="line">[ceph_deploy.mon][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite</span><br><span class="line">[ceph_deploy][ERROR ] GenericError: Failed to create 1 monitors</span><br></pre></td></tr></table></figure>

<p>3）查看监控状态</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ceph health</span><br><span class="line">HEALTH_OK</span><br></pre></td></tr></table></figure>

<p>4）将配置信息同步到所有节点</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ceph-deploy admin ceph01 ceph02 ceph03</span><br><span class="line"><span class="comment"># 查看其他节点是否多了几个配置文件</span></span><br><span class="line">$ ll</span><br><span class="line">总用量 12</span><br><span class="line">-rw------- 1 root root 151 8月   3 14:46 ceph.client.admin.keyring</span><br><span class="line">-rw-r--r-- 1 root root 322 8月   3 14:46 ceph.conf</span><br><span class="line">-rw-r--r-- 1 root root  92 4月  24 01:07 rbdmap</span><br><span class="line">-rw------- 1 root root   0 8月   1 16:46 tmpDX_w1Z</span><br></pre></td></tr></table></figure>

<p>5）再次查看状态</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    <span class="built_in">id</span>:     a9aa277d-7192-4687-b384-9787b73ece71</span><br><span class="line">    health: HEALTH_OK                     <span class="comment"># 集群监控状态OK</span></span><br><span class="line">  </span><br><span class="line">  services:</span><br><span class="line">    mon: 1 daemons, quorum node1 (age 3m) <span class="comment"># 1个mon</span></span><br><span class="line">    mgr: no daemons active</span><br><span class="line">    osd: 0 osds: 0 up, 0 <span class="keyword">in</span></span><br><span class="line">  </span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0 objects, 0 B</span><br><span class="line">    usage:   0 B used, 0 B / 0 B avail</span><br><span class="line">    pgs:    </span><br></pre></td></tr></table></figure>

<p><strong>为了防止 mon 单点故障，你可以加多个 mon 节点(建议奇数个，因为有 quorum 仲裁投票）</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ceph-deploy mon add ceph02</span><br><span class="line">$ ceph-deploy mon add ceph03</span><br><span class="line">$ ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    <span class="built_in">id</span>:     a9aa277d-7192-4687-b384-9787b73ece71</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line">  </span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum node1,node2,node3 (age 0.188542s) <span class="comment"># 3个mon了</span></span><br><span class="line">    mgr: no daemons active</span><br><span class="line">    osd: 0 osds: 0 up, 0 <span class="keyword">in</span></span><br><span class="line">  </span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0 objects, 0 B</span><br><span class="line">    usage:   0 B used, 0 B / 0 B avail</span><br><span class="line">    pgs:    </span><br></pre></td></tr></table></figure>

<p><strong>注：监控异常问题</strong>，ceph 集群对时间同步要求非常高, 即使你已经将 ntpd 服务开启,但仍然可能有 <code>clock skew deteted</code> 相关警告</p>
<p>监控到时间不同步的解决方法：</p>
<p>1）<strong>在 ceph 集群所有节点上(<code>node1</code>,<code>node2</code>,<code>node3</code>)操作，</strong>不使用 ntpd 服务，直接使用 crontab 同步</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ systemctl stop ntpd</span><br><span class="line">$ systemctl <span class="built_in">disable</span> ntpd</span><br><span class="line">$ crontab -e</span><br><span class="line">*/10 * * * * ntpdate ntp1.aliyun.com  <span class="comment"># 每5或10分钟同步1次公网的任意时间服务器</span></span><br></pre></td></tr></table></figure>

<p>2）调大时间警告的阈值</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ vim /etc/ceph/ceph.conf</span><br><span class="line">[global]                          <span class="comment"># 在global参数组里添加以下两行                        </span></span><br><span class="line">mon clock drift allowed = 2       <span class="comment"># monitor间的时钟滴答数(默认0.5秒)</span></span><br><span class="line">mon clock drift warn backoff = 30 <span class="comment"># 调大时钟允许的偏移量(默认为5)</span></span><br></pre></td></tr></table></figure>

<p><font color=red>每次修改了 ceph.conf 配置需要加 <code>--overwrite-conf</code> 参数覆盖同步到所有节点，再所有 ceph 集群节点上重启 ceph-mon.target 服务</font></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ceph-deploy --overwrite-conf admin node1 node2 node3</span><br><span class="line">$ systemctl restart ceph-mon.target</span><br></pre></td></tr></table></figure>

<h3 id="创建-mgr-管理"><a href="#创建-mgr-管理" class="headerlink" title="创建 mgr 管理"></a>创建 mgr 管理</h3><p>eph luminous 版本中新增加了一个组件：Ceph Manager Daemon，简称 ceph-mgr；</p>
<p>该组件的主要作用是分担和扩展 monitor 的部分功能，减轻 monitor 的负担，让更好地管理 ceph 存储系统；</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ceph-deploy mgr create ceph01</span><br></pre></td></tr></table></figure>

<p>添加多个 mgr 可以实现 HA</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ceph-deploy mgr create ceph02</span><br><span class="line">$ ceph-deploy mgr create ceph03</span><br><span class="line">$ ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    <span class="built_in">id</span>:     a9aa277d-7192-4687-b384-9787b73ece71</span><br><span class="line">    health: HEALTH_WARN</span><br><span class="line">            Module <span class="string">&#x27;restful&#x27;</span> has failed dependency: No module named <span class="string">&#x27;pecan&#x27;</span></span><br><span class="line">            Reduced data availability: 1 pg inactive</span><br><span class="line">            OSD count 0 &lt; osd_pool_default_size 3</span><br><span class="line">  </span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum node1,node2,node3 (age 11m)</span><br><span class="line">    mgr: node1(active, since 3m), standbys: node2, node3  <span class="comment"># 3个mgr其中node1为active node2 3为standbys</span></span><br><span class="line">    osd: 0 osds: 0 up, 0 <span class="keyword">in</span></span><br><span class="line">  </span><br><span class="line">  data:</span><br><span class="line">    pools:   1 pools, 1 pgs</span><br><span class="line">    objects: 0 objects, 0 B</span><br><span class="line">    usage:   0 B used, 0 B / 0 B avail</span><br><span class="line">    pgs:     100.000% pgs unknown</span><br><span class="line">             1 unknown</span><br></pre></td></tr></table></figure>

<h3 id="创建-osd-存储"><a href="#创建-osd-存储" class="headerlink" title="创建 osd 存储"></a>创建 osd 存储</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看相关帮忙,lsblk 查看已添加的磁盘情况</span></span><br><span class="line">$ ceph-deploy disk --<span class="built_in">help</span></span><br><span class="line">$ ceph-deploy osd --<span class="built_in">help</span></span><br></pre></td></tr></table></figure>

<p>zap 表示干掉磁盘上的数据,相当于格式化</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ceph-deploy disk zap ceph01 /dev/sdb</span><br><span class="line">$ ceph-deploy disk zap ceph02 /dev/sdb</span><br><span class="line">$ ceph-deploy disk zap ceph03 /dev/sdb</span><br></pre></td></tr></table></figure>

<p>将磁盘创建为 osd</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ceph-deploy osd create --data /dev/sdb ceph01</span><br><span class="line">$ ceph-deploy osd create --data /dev/sdb ceph02</span><br><span class="line">$ ceph-deploy osd create --data /dev/sdb ceph03</span><br><span class="line">$ ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    <span class="built_in">id</span>:     a9aa277d-7192-4687-b384-9787b73ece71</span><br><span class="line">    health: HEALTH_WARN</span><br><span class="line">            Module <span class="string">&#x27;restful&#x27;</span> has failed dependency: No module named <span class="string">&#x27;pecan&#x27;</span></span><br><span class="line">  </span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum ceph01,ceph02,ceph03 (age 22m)</span><br><span class="line">    mgr: ceph01(active, since 14m), standbys: ceph02, ceph03</span><br><span class="line">    osd: 3 osds: 3 up (since 4s), 3 <span class="keyword">in</span> (since 4s) <span class="comment"># 3个osd 3个up启用状态 3 in代表3G在使用 每块硬盘占用1G</span></span><br><span class="line">  </span><br><span class="line">  data:</span><br><span class="line">    pools:   1 pools, 1 pgs</span><br><span class="line">    objects: 0 objects, 0 B</span><br><span class="line">    usage:   3.0 GiB used, 45 GiB / 48 GiB avail  <span class="comment"># 3G使用 45G空闲 总计48G及三块16G硬盘相加之和</span></span><br><span class="line">    pgs:     1 active+clean</span><br></pre></td></tr></table></figure>

<h3 id="集群扩容"><a href="#集群扩容" class="headerlink" title="集群扩容"></a>集群扩容</h3><p>假设再加一个新的集群节点 ceph04</p>
<ol>
<li>主机名配置和绑定</li>
<li>在 ceph04 上 <code>yum install ceph ceph-radosgw -y</code> 安装软件</li>
<li>在部署节点 ceph01 上同步配置文件给 ceph04，<code>ceph-deploy admin ceph04</code></li>
<li>按需求选择在 ceph04 上添加 mon 或 mgr 或 osd 等</li>
</ol>
<h3 id="Ceph-Dashboard"><a href="#Ceph-Dashboard" class="headerlink" title="Ceph-Dashboard"></a>Ceph-Dashboard</h3><p>1）查看集群状态确认 mgr 的 active 节点</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    <span class="built_in">id</span>:     134c6247-2fd4-4669-8c19-d0c9200d90c8</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line">  </span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum ceph01,ceph02,ceph03</span><br><span class="line">    mgr: ceph01(active), standbys: ceph02, ceph03  <span class="comment"># 确认mgr active节点为 ceph01</span></span><br><span class="line">    osd: 3 osds: 3 up, 3 <span class="keyword">in</span></span><br><span class="line">    rgw: 1 daemon active</span><br><span class="line">  </span><br><span class="line">  data:</span><br><span class="line">    pools:   4 pools, 32 pgs</span><br><span class="line">    objects: 189  objects, 1.5 KiB</span><br><span class="line">    usage:   3.0 GiB used, 45 GiB / 48 GiB avail</span><br><span class="line">    pgs:     32 active+clean</span><br></pre></td></tr></table></figure>

<p>2）查看开启及关闭的模块</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ceph mgr module <span class="built_in">ls</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;enabled_modules&quot;</span>: [</span><br><span class="line">        <span class="string">&quot;balancer&quot;</span>,</span><br><span class="line">        <span class="string">&quot;crash&quot;</span>,</span><br><span class="line">        <span class="string">&quot;dashboard&quot;</span>,</span><br><span class="line">        <span class="string">&quot;iostat&quot;</span>,</span><br><span class="line">        <span class="string">&quot;restful&quot;</span>,</span><br><span class="line">        <span class="string">&quot;status&quot;</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="string">&quot;disabled_modules&quot;</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;name&quot;</span>: <span class="string">&quot;hello&quot;</span>,</span><br><span class="line">            <span class="string">&quot;can_run&quot;</span>: <span class="literal">true</span>,</span><br><span class="line">            <span class="string">&quot;error_string&quot;</span>: <span class="string">&quot;&quot;</span></span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;name&quot;</span>: <span class="string">&quot;influx&quot;</span>,</span><br><span class="line">            <span class="string">&quot;can_run&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">            <span class="string">&quot;error_string&quot;</span>: <span class="string">&quot;influxdb python module not found&quot;</span></span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;name&quot;</span>: <span class="string">&quot;localpool&quot;</span>,</span><br><span class="line">            <span class="string">&quot;can_run&quot;</span>: <span class="literal">true</span>,</span><br><span class="line">            <span class="string">&quot;error_string&quot;</span>: <span class="string">&quot;&quot;</span></span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;name&quot;</span>: <span class="string">&quot;prometheus&quot;</span>,</span><br><span class="line">            <span class="string">&quot;can_run&quot;</span>: <span class="literal">true</span>,</span><br><span class="line">            <span class="string">&quot;error_string&quot;</span>: <span class="string">&quot;&quot;</span></span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;name&quot;</span>: <span class="string">&quot;selftest&quot;</span>,</span><br><span class="line">            <span class="string">&quot;can_run&quot;</span>: <span class="literal">true</span>,</span><br><span class="line">            <span class="string">&quot;error_string&quot;</span>: <span class="string">&quot;&quot;</span></span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;name&quot;</span>: <span class="string">&quot;smart&quot;</span>,</span><br><span class="line">            <span class="string">&quot;can_run&quot;</span>: <span class="literal">true</span>,</span><br><span class="line">            <span class="string">&quot;error_string&quot;</span>: <span class="string">&quot;&quot;</span></span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;name&quot;</span>: <span class="string">&quot;telegraf&quot;</span>,</span><br><span class="line">            <span class="string">&quot;can_run&quot;</span>: <span class="literal">true</span>,</span><br><span class="line">            <span class="string">&quot;error_string&quot;</span>: <span class="string">&quot;&quot;</span></span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;name&quot;</span>: <span class="string">&quot;telemetry&quot;</span>,</span><br><span class="line">            <span class="string">&quot;can_run&quot;</span>: <span class="literal">true</span>,</span><br><span class="line">            <span class="string">&quot;error_string&quot;</span>: <span class="string">&quot;&quot;</span></span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;name&quot;</span>: <span class="string">&quot;zabbix&quot;</span>,</span><br><span class="line">            <span class="string">&quot;can_run&quot;</span>: <span class="literal">true</span>,</span><br><span class="line">            <span class="string">&quot;error_string&quot;</span>: <span class="string">&quot;&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>3）<strong>开启 dashboard 模块</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ceph mgr module <span class="built_in">enable</span> dashboard</span><br></pre></td></tr></table></figure>

<p>如开启报错，可按下述处理</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ceph mgr module <span class="built_in">enable</span> dashboard</span><br><span class="line">Error ENOENT: all mgr daemons <span class="keyword">do</span> not support module <span class="string">&#x27;dashboard&#x27;</span>, pass --force to force enablement</span><br><span class="line"></span><br><span class="line"><span class="comment"># 需要在每个开启 mgr 的节点安装 ceph-mgr-dashboard</span></span><br><span class="line">$ yum install ceph-mgr-dashboard -y</span><br></pre></td></tr></table></figure>

<p>注意：不能仅仅在 active 节点安装，需要在 standby 节点都安装</p>
<p>4）<strong>创建自签名证书</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ceph dashboard create-self-signed-cert</span><br><span class="line">Self-signed certificate created</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成密钥对，并配置给ceph mgr</span></span><br><span class="line">$ <span class="built_in">mkdir</span> /etc/mgr-dashboard</span><br><span class="line">$ <span class="built_in">cd</span> /etc/mgr-dashboard/</span><br><span class="line">$ openssl req -new -nodes -x509 -subj <span class="string">&quot;/O=IT-ceph/CN=cn&quot;</span> -days 3650 -keyout dashboard.key -out dashboard.crt -extensions v3_ca</span><br><span class="line">dashboard.crt  dashboard.key</span><br></pre></td></tr></table></figure>

<p>5）<strong>配置 mgr services</strong></p>
<p>在 ceph 集群的 active mgr 节点上配置 mgr services</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ceph config <span class="built_in">set</span> mgr mgr/dashboard/server_addr 188.188.4.111</span><br><span class="line">$ ceph config <span class="built_in">set</span> mgr mgr/dashboard/server_port 8080</span><br></pre></td></tr></table></figure>

<p>重启 dashboard 模块,并查看访问地址(注意：不重启查看的端口是默认的8443端口，无法访问)</p>
<p>6）<strong>重启 dashboard 模块</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ceph mgr module <span class="built_in">disable</span> dashboard</span><br><span class="line">$ ceph mgr module <span class="built_in">enable</span> dashboard</span><br><span class="line"><span class="comment"># 查看mgr service</span></span><br><span class="line">$ ceph mgr services</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;dashboard&quot;</span>: <span class="string">&quot;https://192.168.1.101:8080/&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>7）设置访问 web 页面用户名和密码</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ceph dashboard set-login-credentials admin admin</span><br></pre></td></tr></table></figure>

<p>通过本机或其它主机访问 <a target="_blank" rel="noopener" href="https://ip:8080/">https://ip:8080</a></p>
<h3 id="Ceph-文件存储"><a href="#Ceph-文件存储" class="headerlink" title="Ceph 文件存储"></a>Ceph 文件存储</h3><p>要运行 Ceph 文件系统，必须先装只是带一个 mds 的 Ceph 存储集群</p>
<p>Ceph.MDS：为 Ceph 文件存储类型存放元数据 metadata（也就是说 Ceph 块存储和 Ceph 对象存储不使用 MDS）</p>
<p>1）增设配置：在 ceph01 部署节点上修改配置 &#x2F;etc&#x2F;ceph&#x2F;ceph.conf  增加配置</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mon_allow_pool_delete = <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<p>2）同步配置文件，注意修改了配置文件才需要同步，没有修改不需要同步配置文件</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ceph-deploy --overwrite-conf admin ceph01 ceph02 ceph03</span><br></pre></td></tr></table></figure>

<p>3）创建 mds</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ceph-deploy mds create ceph01 ceph02 ceph03</span><br></pre></td></tr></table></figure>

<p>一个 Ceph 文件系统需要至少两个 RADOS 存储池，一个用于数据，一个用于元数据</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ceph osd pool create cephfs_pool 128</span><br><span class="line">pool <span class="string">&#x27;cephfs_pool&#x27;</span> created</span><br><span class="line">$ ceph osd pool create cephfs_metadata 64</span><br><span class="line">pool <span class="string">&#x27;cephfs_metadata&#x27;</span> created</span><br></pre></td></tr></table></figure>

<p>参数解释</p>
<ul>
<li><p>创建 pool 自定义名为 cephfs_pool 用于存储数据 PG 数为 128 (因用于存储数据所以 PG 数较大)</p>
</li>
<li><p>创建 pool 自定义名为 cephfs_metadata 用于存储元数据 PG 数为 64</p>
</li>
<li><p>少于5个OSD则PG数为128，5-10个OSD则PG数为512，10-50个OSD则PG数为1024，更多的OSD需要自行计算</p>
</li>
</ul>
<p>4）查看创建的 pool 详细信息</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ceph osd pool <span class="built_in">ls</span> |grep cephfs</span><br><span class="line">cephfs_pool</span><br><span class="line">cephfs_metadata</span><br><span class="line"></span><br><span class="line">$ ceph osd pool get cephfs_pool all</span><br><span class="line">size: 3</span><br><span class="line">min_size: 2</span><br><span class="line">pg_num: 128</span><br><span class="line">pgp_num: 128</span><br><span class="line">crush_rule: replicated_rule</span><br><span class="line">hashpspool: <span class="literal">true</span></span><br><span class="line">nodelete: <span class="literal">false</span></span><br><span class="line">nopgchange: <span class="literal">false</span></span><br><span class="line">nosizechange: <span class="literal">false</span></span><br><span class="line">write_fadvise_dontneed: <span class="literal">false</span></span><br><span class="line">noscrub: <span class="literal">false</span></span><br><span class="line">nodeep-scrub: <span class="literal">false</span></span><br><span class="line">use_gmt_hitset: 1</span><br><span class="line">fast_read: 0</span><br><span class="line">pg_autoscale_mode: warn</span><br></pre></td></tr></table></figure>

<p>6）创建文件系统</p>
<p>创建 Ceph 文件系统,并确认客户端访问的节点</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ceph fs new cephfs cephfs_metadata cephfs_pool</span><br><span class="line">new fs with metadata pool 10 and data pool 9</span><br></pre></td></tr></table></figure>
<p>7）查看状态</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ceph osd pool <span class="built_in">ls</span></span><br><span class="line">.rgw.root</span><br><span class="line">default.rgw.control</span><br><span class="line">default.rgw.meta</span><br><span class="line">default.rgw.log</span><br><span class="line">cephfs_pool</span><br><span class="line">cephfs_metadata</span><br><span class="line"></span><br><span class="line">$ ceph fs <span class="built_in">ls</span>   <span class="comment"># 确认metadata池和data池        </span></span><br><span class="line">name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_pool ]</span><br><span class="line"></span><br><span class="line">$ ceph mds <span class="built_in">stat</span></span><br><span class="line">...</span><br><span class="line">cephfs-1/1/1 up  &#123;0=ceph03=up:active&#125;, 2 up:standby <span class="comment"># ceph03为active,up状态;其他为standby</span></span><br></pre></td></tr></table></figure>

<p>注：metadata 保存在 ceph03 上</p>
<p>8）Key 文件生成</p>
<p>客户端准备验证 key 文件，ceph 默认启用了 cephx 认证, 所以客户端的挂载必须要验证（ceph.conf 默认配置文件开启）</p>
<p>可在集群节点(ceph01,ceph02,ceph03)上任意一台查看密钥字符串</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">cat</span> /etc/ceph/ceph.client.admin.keyring</span><br><span class="line">$ ceph-authtool -p /etc/ceph/ceph.client.admin.keyring &gt;admin.key</span><br></pre></td></tr></table></figure>

<p>把这个文件放在客户端<code>client /root/admin.key</code>(注意：直接把 key 复制编辑 admin.key 文档可能会在挂载时报错)</p>
<p>9）客户端挂载</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ yum -y install ceph-fuse</span><br><span class="line">$ mount -t ceph ceph01:6789:/ /mnt -o name=admin,secretfile=/root/admin.key</span><br><span class="line">$ <span class="built_in">df</span> -h</span><br></pre></td></tr></table></figure>

<p>需要安装否则客户端不支持，也可以使用其他 node 主机名进行挂载，例如 ceph03</p>
<p>注意：如使用文件挂载报错，可以使用参数 secret&#x3D;秘钥 进行挂载；也可以使用两个客户端，同时挂载此文件存储，实现同读同写；</p>
<p>10）验证集群</p>
<p>往挂载的硬盘写数据可以在 dashboard 查看读写监控状态</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">dd</span> <span class="keyword">if</span>=/dev/zero of=/mnt/file1 bs=10M count=1000</span><br></pre></td></tr></table></figure>

<p>11）删除文件存储</p>
<p>在所有挂载了文件存储的客户端卸载文件挂载，并停掉所有节点的 mds</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ umount /mnt</span><br><span class="line">$ systemctl stop ceph-mds.target</span><br></pre></td></tr></table></figure>

<p>回到集群任意一个节点上(node1,node2,node3 其中之一)删除</p>
<p><strong>如果要客户端删除,需要在 node1 上 <code>ceph-deploy admin client</code> 同步配置才可以</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ceph fs <span class="built_in">rm</span> cephfs --yes-i-really-mean-it</span><br><span class="line">$ ceph osd pool delete cephfs_metadata cephfs_metadata --yes-i-really-really-mean-it</span><br><span class="line">pool <span class="string">&#x27;cephfs_metadata&#x27;</span> removed</span><br><span class="line">$ ceph osd pool delete cephfs_pool cephfs_pool --yes-i-really-really-mean-it</span><br><span class="line">pool <span class="string">&#x27;cephfs_pool&#x27;</span> removed</span><br></pre></td></tr></table></figure>

<p>注意：为了安全需要输入两次创建的pool名并且加参数 <code>--yes-i-really-really-mean-it</code> 才能删除</p>
<p>另注意：需要在配置文件添加以下配置，才能删除</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mon_allow_pool_delete = <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<p>如还提示报错：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Error EPERM: pool deletion is disabled; you must first <span class="built_in">set</span> the mon_allow_pool_delete config option to <span class="literal">true</span> before you can destroy a pool</span><br></pre></td></tr></table></figure>

<p>则重启服务 ceph-mon.target 即可<code>$ systemctl restart ceph-mon.target</code></p>
<p>上述执行后，再重新开启所有节点的 mds 服务</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ systemctl start ceph-mds.target</span><br></pre></td></tr></table></figure>

<h3 id="Ceph-块存储"><a href="#Ceph-块存储" class="headerlink" title="Ceph 块存储"></a>Ceph 块存储</h3><h4 id="创建"><a href="#创建" class="headerlink" title="创建"></a>创建</h4><p>1）同步配置：在 ceph01上同步配置文件到 client</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /etc/ceph</span><br><span class="line">$ ceph-deploy admin client</span><br></pre></td></tr></table></figure>

<p>2）建议存储池，并初始化（在客户端 client 操作）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ceph osd pool create rbd_pool 128</span><br><span class="line">pool <span class="string">&#x27;rbd_pool&#x27;</span> created</span><br><span class="line"></span><br><span class="line">$ rbd pool init rbd_pool</span><br></pre></td></tr></table></figure>

<p>3）创建存储卷，一个存储卷（这里卷名为 volume1 大小为 5000M）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ rbd create volume1 --pool rbd_pool --size 5000</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看创建状态</span></span><br><span class="line">$ rbd <span class="built_in">ls</span> rbd_pool</span><br><span class="line">volume1</span><br><span class="line">$ rbd info volume1 -p rbd_pool</span><br><span class="line">rbd image <span class="string">&#x27;volume1&#x27;</span>:         <span class="comment"># volume1 为 rbd_image</span></span><br><span class="line">    size 4.9 GiB <span class="keyword">in</span> 1250 objects</span><br><span class="line">    order 22 (4 MiB objects)</span><br><span class="line">    <span class="built_in">id</span>: 60fc6b8b4567</span><br><span class="line">    block_name_prefix: rbd_data.60fc6b8b4567</span><br><span class="line">    format: 2                <span class="comment"># 两种格式1和2 默认是2</span></span><br><span class="line">    features: layering, exclusive-lock, object-map, fast-diff, deep-flatten</span><br><span class="line">    op_features:</span><br><span class="line">    flags:</span><br><span class="line">    create_timestamp: Tue Aug  4 09:20:05 2020</span><br></pre></td></tr></table></figure>

<p>4）映射块设备，将创建的卷映射成块设备，因为 rbd 镜像的一些特性，OS kernel 并不支持，所以映射报错</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ rbd map rbd_pool/volume1</span><br><span class="line">rbd: sysfs write failed</span><br><span class="line">RBD image feature <span class="built_in">set</span> mismatch. You can <span class="built_in">disable</span> features unsupported by the kernel with <span class="string">&quot;rbd feature disable rbd_pool/volume1 object-map fast-diff deep-flatten&quot;</span>.</span><br><span class="line">In some cases useful info is found <span class="keyword">in</span> syslog - try <span class="string">&quot;dmesg | tail&quot;</span>.</span><br><span class="line">rbd: map failed: (6) No such device or address</span><br></pre></td></tr></table></figure>

<p><strong>解决办法：disable 掉相关特性</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ rbd feature <span class="built_in">disable</span> rbd_pool/volume1 exclusive-lock object-map fast-diff deep-flatten</span><br></pre></td></tr></table></figure>

<p>再次映射</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ rbd map rbd_pool/volume1</span><br><span class="line">/dev/rbd0</span><br><span class="line"></span><br><span class="line">$ rbd showmapped</span><br><span class="line"><span class="built_in">id</span> pool     image   snap device   </span><br><span class="line">0  rbd_pool volume1 -    /dev/rbd0</span><br></pre></td></tr></table></figure>

<p>创建了磁盘&#x2F;dev&#x2F;rbd0 类似于做了一个软连接</p>
<p><strong>如果需要取消映射可以使用命令</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ rbd unmap /dev/rbd0</span><br></pre></td></tr></table></figure>

<p>5）格式化挂载</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ mkfs.xfs /dev/rbd0</span><br><span class="line">$ mount /dev/rbd0 /mnt</span><br><span class="line">$ <span class="built_in">df</span> -h|<span class="built_in">tail</span> -1</span><br><span class="line">/dev/rbd0                4.9G   33M  4.9G    1% /mnt</span><br></pre></td></tr></table></figure>

<h4 id="扩容"><a href="#扩容" class="headerlink" title="扩容"></a>扩容</h4><p>1）如扩容成8000M</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ rbd resize --size 8000 rbd_pool/volume1</span><br><span class="line">Resizing image: 100% complete...done.</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看并没有变化</span></span><br><span class="line">$ <span class="built_in">df</span> -h|<span class="built_in">tail</span> -1</span><br><span class="line">/dev/rbd0                4.9G   33M  4.9G    1% /mnt</span><br></pre></td></tr></table></figure>

<p>2）动态刷新扩容</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ xfs_growfs -d /mnt/</span><br><span class="line">meta-data=/dev/rbd0              isize=512    agcount=8, agsize=160768 blks</span><br><span class="line">         =                       sectsz=512   attr=2, projid32bit=1</span><br><span class="line">         =                       crc=1        finobt=0 spinodes=0</span><br><span class="line">data     =                       bsize=4096   blocks=1280000, imaxpct=25</span><br><span class="line">         =                       sunit=1024   swidth=1024 blks</span><br><span class="line">naming   =version 2              bsize=4096   ascii-ci=0 ftype=1</span><br><span class="line"><span class="built_in">log</span>      =internal               bsize=4096   blocks=2560, version=2</span><br><span class="line">         =                       sectsz=512   sunit=8 blks, lazy-count=1</span><br><span class="line">realtime =none                   extsz=4096   blocks=0, rtextents=0</span><br><span class="line">data blocks changed from 1280000 to 2048000</span><br><span class="line"></span><br><span class="line"><span class="comment"># 再次查看，扩容成功</span></span><br><span class="line">$ <span class="built_in">df</span> -h|<span class="built_in">tail</span> -1</span><br><span class="line">/dev/rbd0                7.9G   33M  7.8G    1% /mnt</span><br></pre></td></tr></table></figure>

<p>注意：该命令和 LVM 扩容命令一致</p>
<h4 id="缩容"><a href="#缩容" class="headerlink" title="缩容"></a>缩容</h4><p>1）<strong>块存储裁减</strong>不能在线裁减，裁减后需要重新格式化再挂载，如果有数据需要提前备份好数据，如裁减为5000M</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ rbd resize --size 5000 rbd_pool/volume1 --allow-shrink</span><br><span class="line">Resizing image: 100% complete...done.</span><br></pre></td></tr></table></figure>

<p>2）卸载，格式化</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ umount /mnt</span><br><span class="line">$ fdisk -l</span><br><span class="line">$ mkfs.xfs -f /dev/rbd0</span><br><span class="line">$ mount /dev/rbd0 /mnt/</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">df</span> -h|<span class="built_in">tail</span> -1</span><br><span class="line">/dev/rbd0                4.9G   33M  4.9G    1% /mnt</span><br></pre></td></tr></table></figure>

<h4 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#卸载</span></span><br><span class="line">$ umount /mnt</span><br><span class="line"><span class="comment">#取消映射</span></span><br><span class="line">$ rbd unmap /dev/rbd0</span><br><span class="line"><span class="comment">#删除存储池</span></span><br><span class="line">$ ceph osd pool delete rbd_pool rbd_pool --yes-i-really-really-mean-it</span><br><span class="line">pool <span class="string">&#x27;rbd_pool&#x27;</span> removed</span><br></pre></td></tr></table></figure>

<h3 id="Ceph-对象存储"><a href="#Ceph-对象存储" class="headerlink" title="Ceph 对象存储"></a>Ceph 对象存储</h3><p>1）创建 rgw（在 ceph01 上创建 rgw）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ceph-deploy rgw create ceph01</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看,运行端口是7480</span></span><br><span class="line">$ lsof -i:7480</span><br></pre></td></tr></table></figure>

<p>2）测试连接，在客户端测试连接对象网关，在 client 安装测试工具，创建一个测试用户，需要在部署节点使用 <code>ceph-deploy admin client</code> 同步配置文件给 client</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ radosgw-admin user create --uid=<span class="string">&quot;testuser&quot;</span> --display-name=<span class="string">&quot;First User&quot;</span>|grep -E <span class="string">&#x27;access_key|secret_key&#x27;</span></span><br><span class="line">            <span class="string">&quot;access_key&quot;</span>: <span class="string">&quot;S859J38AS6WW1CZSB90M&quot;</span>,</span><br><span class="line">            <span class="string">&quot;secret_key&quot;</span>: <span class="string">&quot;PCmfHoAsHw2GIEioSWvN887o02VXesOkX2gJ20fG&quot;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ radosgw-admin user create --uid=<span class="string">&quot;testuser&quot;</span> --display-name=<span class="string">&quot;First User&quot;</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;user_id&quot;</span>: <span class="string">&quot;testuser&quot;</span>,</span><br><span class="line">    <span class="string">&quot;display_name&quot;</span>: <span class="string">&quot;First User&quot;</span>,</span><br><span class="line">    <span class="string">&quot;email&quot;</span>: <span class="string">&quot;&quot;</span>,</span><br><span class="line">    <span class="string">&quot;suspended&quot;</span>: 0,</span><br><span class="line">    <span class="string">&quot;max_buckets&quot;</span>: 1000,</span><br><span class="line">    <span class="string">&quot;auid&quot;</span>: 0,</span><br><span class="line">    <span class="string">&quot;subusers&quot;</span>: [],</span><br><span class="line">    <span class="string">&quot;keys&quot;</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;user&quot;</span>: <span class="string">&quot;testuser&quot;</span>,</span><br><span class="line">            <span class="string">&quot;access_key&quot;</span>: <span class="string">&quot;S859J38AS6WW1CZSB90M&quot;</span>,</span><br><span class="line">            <span class="string">&quot;secret_key&quot;</span>: <span class="string">&quot;PCmfHoAsHw2GIEioSWvN887o02VXesOkX2gJ20fG&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">&quot;swift_keys&quot;</span>: [],</span><br><span class="line">    <span class="string">&quot;caps&quot;</span>: [],</span><br><span class="line">    <span class="string">&quot;op_mask&quot;</span>: <span class="string">&quot;read, write, delete&quot;</span>,</span><br><span class="line">    <span class="string">&quot;default_placement&quot;</span>: <span class="string">&quot;&quot;</span>,</span><br><span class="line">    <span class="string">&quot;placement_tags&quot;</span>: [],</span><br><span class="line">    <span class="string">&quot;bucket_quota&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;enabled&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="string">&quot;check_on_raw&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="string">&quot;max_size&quot;</span>: -1,</span><br><span class="line">        <span class="string">&quot;max_size_kb&quot;</span>: 0,</span><br><span class="line">        <span class="string">&quot;max_objects&quot;</span>: -1</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&quot;user_quota&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;enabled&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="string">&quot;check_on_raw&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="string">&quot;max_size&quot;</span>: -1,</span><br><span class="line">        <span class="string">&quot;max_size_kb&quot;</span>: 0,</span><br><span class="line">        <span class="string">&quot;max_objects&quot;</span>: -1</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&quot;temp_url_keys&quot;</span>: [],</span><br><span class="line">    <span class="string">&quot;type&quot;</span>: <span class="string">&quot;rgw&quot;</span>,</span><br><span class="line">    <span class="string">&quot;mfa_ids&quot;</span>: []</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上面一大段主要有用的为 access_key 与 secret_key，用于连接对象存储网关</p>
<p>3）S3连接对象网关，客户端安装 s3cmd 工具，并编写配置文件</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ yum install s3cmd</span><br></pre></td></tr></table></figure>

<p>4）创建配置文件，内容如下</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">cat</span> /root/.s3cfg</span><br><span class="line">[default]</span><br><span class="line">access_key = S859J38AS6WW1CZSB90M</span><br><span class="line">secret_key = PCmfHoAsHw2GIEioSWvN887o02VXesOkX2gJ20fG</span><br><span class="line">host_base = 192.168.1.101:7480</span><br><span class="line">host_bucket = 192.168.1.101:7480/%(bucket)</span><br><span class="line">cloudfront_host = 192.168.1.101:7480</span><br><span class="line">use_https = False</span><br></pre></td></tr></table></figure>

<p>5）列出 bucket</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ s3cmd <span class="built_in">ls</span></span><br></pre></td></tr></table></figure>

<p>6）创建一个桶，上传、下载测试</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ s3cmd mb s3://test_bucket              <span class="comment"># 创建</span></span><br><span class="line">$ s3cmd put /etc/fstab s3://test_bucket  <span class="comment"># 上传</span></span><br><span class="line">$ s3cmd get  s3://test_bucket/fstab      <span class="comment"># 下载</span></span><br></pre></td></tr></table></figure>



</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">YuiKuen.Yuen</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://blog.yuikuen.top/2022/04/07/ceph/centos7部署_Ceph集群/">https://blog.yuikuen.top/2022/04/07/ceph/centos7部署_Ceph集群/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/ceph/">ceph</a></div><div class="post-qr-code"><div class="post-qr-code-item"><img class="post-qr-code__img" src="https://cdn.jsdelivr.net/gh/yuikuen/picgo-cdn_images/img/qq-s.png"><div class="post-qr-code__desc">支付宝打赏</div></div><div class="post-qr-code-item"><img class="post-qr-code__img" src="https://cdn.jsdelivr.net/gh/yuikuen/picgo-cdn_images/img/wechat-s.png"><div class="post-qr-code__desc">微信打赏</div></div></div><div class="addthis_inline_share_toolbox pull-right"></div><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-624e4db30419cf74" async></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2022/04/07/ceph/rook%E9%83%A8%E7%BD%B2_Ceph%E9%9B%86%E7%BE%A4/"><i class="fa fa-chevron-left">  </i><span>Rook部署_Ceph集群</span></a></div><div class="next-post pull-right"><a href="/2022/04/07/centos/centos7%E9%97%AE%E9%A2%98_%E6%B8%85%E9%99%A4buff&amp;cache/"><span>CentOS7问题_清除buff&amp;cache</span><i class="fa fa-chevron-right"></i></a></div></nav><div class="post-adv"><a target="_blank" rel="noopener" href="https://www.vultr.com/"><img src="https://www.vultr.com/media/banner_1.png" width="728" height="90"></a></div><div id="vcomment"></div><script src="https://cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var notify = 'false' == 'true';
var verify = 'false' == 'true';
var record_ip = 'false' == 'true';
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  recordIP:record_ip,
  appId:'51vOu1zj5thMsDwfiYj0Gr34-gzGzoHsz',
  appKey:'XPMz9Uun0aSLDkHrDVAlmy8Q',
  placeholder:'Please enter your opinion',
  avatar:'mm',
  guest_info:guest_info,
  pageSize:'10',
  lang: 'zh-cn'
})</script></div></div><footer class="footer-bg" style="background-image: url(https://cdn.jsdelivr.net/gh/yuikuen/picgo-cdn_images/img/wallhaven-wqx3e7-tuya.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2021 - 2022 By YuiKuen.Yuen</div><div class="framework-info"><span>Driven - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://blog.yuikuen.top/">blog</a>!</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script id="ribbon" src="/js/third-party/canvas-ribbon.js" size="150" alpha="0.6" zIndex="-1" data-click="true"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>